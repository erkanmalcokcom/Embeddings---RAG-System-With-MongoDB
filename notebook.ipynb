{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MongoDB, a widely adopted NoSQL document database, plays a pivotal role in the RAG system. It efficiently stores and retrieves embeddings and associated text data. MongoDB's adaptability and scalability make it an ideal choice for managing large text corpora and their corresponding embeddings in the RAG system.\n",
    "\n",
    "In a RAG system integrated with MongoDB, the embeddings and text data are stored in a MongoDB collection. Each document in the collection represents a text passage or document. The embeddings can be stored within the document as arrays or binary data. This storage method enables efficient nearest neighbour search using MongoDB's geospatial indexing capabilities, a crucial aspect of the RAG system's functionality.\n",
    "\n",
    "During the retrieval phase, the input text would be encoded into an embedding, and MongoDB's geospatial queries (e.g., $nearSphere) can be used to find the nearest neighbour embeddings in the database, effectively retrieving the most relevant documents or passages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets pandas openai pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataset(\"MongoDB/embedded_movies\", limit=10)\n",
    "\n",
    "# Convert the dataset to a pandas dataframe\n",
    "df = pd.DataFrame(df['train'])\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_cmd = ['df.columns', 'df.shape', 'df.describe()', 'df.isnull().sum()']\n",
    "\n",
    "for cmd in lst_cmd:\n",
    "    print(eval(cmd), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['plot'])\n",
    "df = df.drop(columns=['plot_embedding'])\n",
    "\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "import openai\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if openai.api_key is None:\n",
    "    raise Exception(\"API key is required. Set OPENAI_API_KEY environment variable.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text):\n",
    "    \"\"\"Generate an embedding for the given text using OpenAI's API.\"\"\"\n",
    "\n",
    "    # Check for valid input\n",
    "    if not text or not isinstance(text, str):\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Call OpenAI API to get the embedding\n",
    "        embedding = client.embeddings.create(input=text, model=\"text-embedding-3-small\").data[0].embedding\n",
    "        return embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Error in get_embedding: {e}\")\n",
    "        return None\n",
    "\n",
    "df[\"plot_embedding_optimised\"] = df['plot'].apply(get_embedding)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "\n",
    "uri = \"mongodb+srv://erkanmalcok:2LtkNVWOuWnmSP9c@cluster0.grz4hfh.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
    "\n",
    "# Create a new client and connect to the server\n",
    "mongo_client = MongoClient(uri, server_api=ServerApi('1'))\n",
    "\n",
    "# Send a ping to confirm a successful connection\n",
    "try:\n",
    "    mongo_client.admin.command('ping')\n",
    "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = mongo_client['movies']\n",
    "collection = db['movie_collection']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.delete_many({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = df.to_dict('records')\n",
    "collection.insert_many(documents)\n",
    "\n",
    "print(\"Data ingestion into MongoDB completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search(user_query, collection):\n",
    "    \"\"\"\n",
    "    Perform a vector search in the MongoDB collection based on the user query.\n",
    "\n",
    "    Args:\n",
    "    user_query (str): The user's query string.\n",
    "    collection (MongoCollection): The MongoDB collection to search.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of matching documents.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate embedding for the user query\n",
    "    query_embedding = get_embedding(user_query)\n",
    "\n",
    "    if query_embedding is None:\n",
    "        return \"Invalid query or embedding generation failed.\"\n",
    "\n",
    "    # Define the vector search pipeline\n",
    "    pipeline = [\n",
    "        {\n",
    "            \"$vectorSearch\": {\n",
    "                \"index\": \"vector_index\",\n",
    "                \"queryVector\": query_embedding,\n",
    "                \"path\": \"plot_embedding_optimised\",\n",
    "                \"numCandidates\": 150,  # Number of candidate matches to consider\n",
    "                \"limit\": 5  # Return top 5 matches\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"$project\": {\n",
    "                \"_id\": 0,  # Exclude the _id field\n",
    "                \"plot\": 1,  # Include the plot field\n",
    "                \"title\": 1,  # Include the title field\n",
    "                \"genres\": 1, # Include the genres field\n",
    "                \"score\": {\n",
    "                    \"$meta\": \"vectorSearchScore\"  # Include the search score\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Execute the search\n",
    "    results = collection.aggregate(pipeline)\n",
    "    return list(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_user_query(query, collection):\n",
    "\n",
    "  get_knowledge = vector_search(query, collection)\n",
    "\n",
    "  search_result = ''\n",
    "  for result in get_knowledge:\n",
    "      search_result += f\"Title: {result.get('title', 'N/A')}, Plot: {result.get('plot', 'N/A')}\\n\"\n",
    "\n",
    "  completion = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a movie recommendation system.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Answer this user query: \" + query + \" with the following context: \" + search_result}\n",
    "        ]\n",
    "    )\n",
    "  \n",
    "  print(completion.choices[0].message)\n",
    "  print(search_result)\n",
    "\n",
    "  return (completion.choices[0].message), search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the best sci-fi and adventure movie to watch?\"\n",
    "response, source_information = handle_user_query(query, collection)\n",
    "\n",
    "print(f\"Response: {response}\")\n",
    "# print(f\"Source Information: \\n{source_information}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
